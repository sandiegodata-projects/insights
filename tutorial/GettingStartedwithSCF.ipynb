{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "frontmatter"
    ]
   },
   "source": [
    "show_input: show\n",
    "github: \n",
    "featured_image: \n",
    "authors:\n",
    "- email: eric@civicknowledge.com\n",
    "  name: Eric Busboom\n",
    "  organization: Civic Knowledge\n",
    "  type: Analyst\n",
    "- identifier: abe08fe1-f30b-4412-920c-b0c3e1ffc053\n",
    "tags:\n",
    "- scf\n",
    "- survey\n",
    "- economics\n",
    "categories:\n",
    "- Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rowgenerators as rg\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def markdown(line, cell):\n",
    "    return Markdown(cell.format(**globals()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Title"
    ]
   },
   "source": [
    "## Survey of Consumer Finances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Description"
    ]
   },
   "source": [
    "\n",
    "A tutorial for getting started with the Survey of Consumer Finances, one of the most important surveys for studying American's financial status and habits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Survey of Consumer Finances is a large, cross-sectional, triennial survey of Americans that asks questions about families’ balance sheets, pensions, income, and demographic characteristics. It is sponsored and published by the [Board of Governors of the Federal Reserve System](https://www.federalreserve.gov/econres/scfindex.htm) and administered by [NORC](https://scf.norc.org/index.html). It is one of the most widely used datasets<sup><a href=\"#fnote1\" rel=\"noopener\" target=\"_self\">1</a></sup> for financial research and is a critical resource for studying a wide range of topics. The Federal Reserve has a chartbook application that you can use to get a sense of what data is in the SCF and what you can analyze with it. \n",
    "\n",
    "In this guide we will analyze the full public dataset and the extract files using Python and Pandas. \n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "Nearly all of the files, scripts and documentation for the SCF are available on the web page for each year of the survey. The page for the 2019 survey is: [https://www.federalreserve.gov/econres/scfindex.htm](https://www.federalreserve.gov/econres/scfindex.htm)\n",
    "\n",
    "Other survey pages are linked from the “[Previous Surveys](https://www.federalreserve.gov/econres/scf-previous-surveys.htm)” link in the left hand menu.\n",
    "\n",
    "The main download page has many links to files and documentation in a variety of formats, and each collection of files is organized in a blue table. The first table, in the “Historic Tables” section has compiled data from multiple years. These files are useful for basic reporting, but not for other types of analysis. We are interested in the Full Public Dataset and the Summary Extract Public Data. We’ll access these files in the Stata format using Pandas.\n",
    "\n",
    "For our analysis, there are three important data files: \n",
    "\n",
    "* The Stata version of the [Full Public Data Set ](https://www.federalreserve.gov/econres/files/scf2019s.zip)\n",
    "* The Stata version of the [Summary Extract Public Data](https://www.federalreserve.gov/econres/files/scfp2019s.zip)\n",
    "* The Stata version of the [Replicate Weight Fille](https://www.federalreserve.gov/econres/files/scf2019rw1s.zip)\n",
    "\n",
    "There are also two import documentation files:\n",
    "\n",
    "* The [Codebook](https://www.federalreserve.gov/econres/files/codebk2019.txt), which describes the variables in the Main Survey Data.\n",
    "* The [SAS macro](https://www.federalreserve.gov/econres/files/bulletin.macro.txt) that creates the Summary Extract Public Data. This macro code the only documentation for the extract file variables.\n",
    "\n",
    "The SCF is a very complex dataset, with many quirks that researchers should be aware of. The Hanna et al article<sup><a href=\"#fnote1\" rel=\"noopener\" target=\"_self\">1</a></sup> provides an authoritative discussion of the nuances of this dataset.  \n",
    "\n",
    "#### Dataset Structure and Organization. \n",
    "\n",
    "The SCF appears to collect financial information about households, but the main unit of analysis is actually the \"primary economic unit\"<sup><a href=\"#fnote2\" rel=\"noopener\" target=\"_self\">2</a></sup> (PEU), which is defined as an economically dominant individual or coupe and all of the people dependent on that individual or couple. This unit may be a subset of the household if some members of the household are independently employed. However, most documentation will use the terms \"household\" and \"primary economic unit\" interchangbly, and sometimes even SCF staff will use the term \"family\" instead. For convenience, we will use \"household\" to describe the primary economic units. \n",
    "\n",
    "Because the dataset is a survey, it does not have responses from every household in America, so in order to use the dataset to infer statistics about the whole American population, each record represents around 20,000 American households. The number of households that a record represents is known as the weight; the sum of the weights in the dataset should equal the number of households in the US. \n",
    "\n",
    "However, the weights for each record are statistical guesses, and there could be other possible weights for each household. The main data file holds one primary weight, but the weights file holds an additional 999 weights -- the replicate weights -- that can be used for more complex analsys. We will use the main weight in this tutorial, and demonstrate how to use the replicate weights later. \n",
    "\n",
    "But this file has one additional set of weights -- the implicate weights. The data file, for 2019, has records for 6248, but the data file has 5 times as many rows, 5 rows for each household. Each of these rows is an implicate, a repetition of the reocord that serves to obscure the identity of the household and to provide a statistically sophisticate way of handling missing values. The implicates will be discussed later, for now we will use only the first implicate and ignore the rest. \n",
    "\n",
    "There are two identifier values for the records. Variable ``YY1`` identifies each household, and because of the implicates, each value will be repeated 5 times. The second inentifier is ``Y1``, which is constructed by multiplying ``YY1`` by 10 and adding the implicate number\n",
    "\n",
    "The main file, the Full Publilc Data set, holds all of the publicly released variables, but these variables have cryptic names. For instance, ``X5907`` is the variable for the respondent's date of birth. The second important file, the Summary Extract Public Data, is created from the primary data with an SAS macro. It's variables have human readable names; ``HHSEX`` is the variable for the sex of the household reference person. Additionally, the macro maintains the same variable names from year to year, with approximately the same meaning, so using this file is often much easier. For instance, the main dataset has many variables for household income, and often, a household's income can vary a lot from year to year. In the Extract file, all of these values are combined and smoothed in the ``NORMINC`` ( Normal annual income ) variable. Using this file can greatly reduce an analysts burden of data preparation. \n",
    "\n",
    "\n",
    "### Loading Files\n",
    "\n",
    "To load the files, we will use the [rowgenerators](https://pypi.org/project/rowgenerators/) Python package which provides support for getting Pandas Dataframes from urls. The rowgenerators package will cache the download files, so re-running our program is faster, and can select a specific file out of the ZIP archive. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rowgenerators as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The  row generators package can extract the .dta file from the ZIP archive and\n",
    "# produce a dataframe. It also caches them locally, to speed-up re-runs.\n",
    "\n",
    "frb_site = 'https://www.federalreserve.gov/econres/files'\n",
    "\n",
    "# Public File\n",
    "scfp = rg.dataframe(frb_site+'/scf2016s.zip#p16i6.dta')\n",
    "\n",
    "# Extract\n",
    "scfe = rg.dataframe(frb_site+'/scfp2016s.zip#rscfp2016.dta')\n",
    "\n",
    "# Weights\n",
    "scfw = rg.dataframe(frb_site+'/scf2016rw1s.zip#p16_rw1.dta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The public file is fairly large: 31K rows, and 5K columns, and the columns names are nearly all cryptic codes, which you wil have to decipher with the [Codebook.](https://www.federalreserve.gov/econres/files/codebk2019.txt). By the way, the Codebook is 850 pages, so don't try to print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31240 entries, 0 to 31239\n",
      "Columns: 5320 entries, J7398 to X11572\n",
      "dtypes: float32(293), float64(103), int16(2526), int32(376), int8(2022)\n",
      "memory usage: 315.3 MB\n"
     ]
    }
   ],
   "source": [
    "scfp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Summary Extract Public Data file is different. It is just as long as the main public file, with one row per row of the main file, but only has 348 columns, and the columns have sensible names. There doesn't appear to be a codebook for this file. Instead, all of the columns are created by a SAS script, and you [have to read the script](https://www.federalreserve.gov/econres/files/bulletin.macro.txt) to understand what the variables are for. Fortunately, the script is not hard to read. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31240 entries, 0 to 31239\n",
      "Columns: 348 entries, YY1 to nincpctlecat\n",
      "dtypes: float64(82), int16(21), int32(61), int8(184)\n",
      "memory usage: 33.8 MB\n"
     ]
    }
   ],
   "source": [
    " scfe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Analysis\n",
    "\n",
    "The plan for our analysis is to first create an extract file that we'll use for all of the later analysis, with sensibe names for the variables, then perform the analysis. Most often, these steps are in seperate Jupyter notebooks, but here we will join them together. \n",
    "\n",
    "Extracting data has a few steps. First we have to locate the variables that we want to use in the codebook and extract them from the file. Since all of the variable names in the mail file are codes, we will want to rename them to more memorable names. The values from the Extract file already have somewhat sensible names, so we will extract those variabes without renaming them. We will also have to either extract only the first of the implicate, or will have to remember to divide sums of records by 5, or divide the weights by 5. But, there is also one other option, if we have a lot of memory, that makes the analysis simpler. \n",
    "\n",
    "Because the records are weighted -- each household actually represents from between 10 and 65,000 real households -- any summary statistics, such as means or medians, will have to account for the weighting. Because of the structure of the sampling for the SCF, proper weighting is very important. \n",
    "\n",
    "However, there is another technique that can avoid many of the dificulties and produce reasonably accurate estimates with no complicated hadling of weights: sampling. We will do analysis in this tutorial using sampling, and demonstrate other handling of weights later. The idea behind sampling is very simple: we select a large number of records from the dataset, 100,000 or more, with a probability of selection equal to the record weight. With a large number of sampled records, we can easily compute percentages, ratios, means and medians. \n",
    "\n",
    "#### Extracting Records. \n",
    "\n",
    "Our first step is to extract the records we will need for analysis. For the main file, we will need to consult the code book to get the variabe code, and then we'll want to assign it a sensible name. This information is best captured in a Python dict: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92b0fbe1b20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdf_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdf_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'implicate_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_id\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "p_cols = {\n",
    "    'case_id': 'YY1',\n",
    "    'record_id': 'Y1',\n",
    "    'age_1': 'X14', # Reconciled age\n",
    "    'age_2': 'X19', # Reconciled age\n",
    "    'hisp': 'X7004', # Do you consider yourself to be Hispanic or Latino in culture or origin?\n",
    "    'race': 'X6809', # Race of respondent\n",
    "    'addtional_race': 'X6810', # Respondent offered another race categot (1) or did not (5)\n",
    "    # X6402 #In 2015, did (other adult) receive any income from wages or salaries?\n",
    "    #'income': 'X5729', # How much was the total income you (and your family living here) received in 2015 from all sources, before taxes and other deductions were made?\n",
    "    'unusual_income': 'X7650', # Is this income unusually high or low...\n",
    "    'ed_1': 'X5931', # What is the highest level of school completed or the highest degree you have received?\n",
    "    'ed_2': 'X6111', # What is the highest level of school completed or the highest degree you have received?\n",
    "    'ed_mother_1': 'X6032', # What is the highest level of school or the highest degree mother completed?\n",
    "    'ed_father_1': 'X6033', # What is the highest level of school or the highest degree father completed?\n",
    "    'ed_mother_2': 'X6132', # What is the highest level of school or the highest degree mother completed?\n",
    "    'ed_father_2': 'X6133', # What is the highest level of school or the highest degree father completed?  \n",
    "    'occ_1': 'X7401', # What is the official title of your job?\n",
    "    'occ_2': 'X7411', # What is the official title of your job?\n",
    "    'gi_other_value': 'X5818', #How much altogether were any others (inheritances) you have received?\n",
    "    'fin_risk': 'X7557', # Willingness to take fnancial risks, 1 to 10\n",
    "    'shop_credit' : 'X7561', # Financial shopping\n",
    "    'shop_credit_2' : 'X7562', # Financial Shopping\n",
    "    'fin_know': 'X7556', # Financial Knowledge\n",
    "    'borrow_vacation': 'X402', # Borrow for a vacation\n",
    "    'plan_horizon': 'X3008', # which of the time periods listed on this page is most important to you\n",
    "    'spend_exceeds':'X7510', # would you say that your (family's) spending exceeded your (family's) income,\n",
    "    'spend_exceeds_excl_house':'X7508', # Spending exceeds, after purchase of house. \n",
    "    'wt0': 'X42001' # Weight   \n",
    "}\n",
    " \n",
    "df_p = scfp[p_cols.values()].rename(columns={v:k for k,v in p_cols.items()})\n",
    "df_p.insert(2, 'implicate_id', df.record_id - df.case_id.astype('int32')*10)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The variables from the Public Extract file are easier to use, because they already have sensible names, so they can be collected into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_cols = ['y1',\n",
    "'networth',  'income', 'nwcat', 'nwpctlecat', \n",
    "'norminc', 'ninccat',  'ninc2cat', 'nincpctlecat',\n",
    "'occat1', 'occat2', 'edcl', 'lifecl', \n",
    "'famstruct', 'married',  'agecl', 'housecl', \n",
    "'racecl','racecl4',\n",
    "'asset', 'liq','bond', 'fin','nfin', 'debt', \n",
    "'indcat', 'equity','homeeq', 'revpay','bnkruplast5', \n",
    "'debt2inc', 'hsaving' , 'saved' ]\n",
    "\n",
    "scfe.columns = [c.lower() for c in scfe.columns] # Lowercase so all cols are consistent case\n",
    "\n",
    "df_e = scfe[e_cols].rename(columns={'y1':'record_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_p.merge(df_e, on='record_id')\n",
    "df.wt0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting and Sampling\n",
    "\n",
    "The SCL is a weighted dataset: each record represents more than one household in the US, and there are multiple columns that indicate how many housholds each record can represent. We will focus on the ``wt0`` column, the primary weight. As with most weighted datasets, adding up the weights for all of the records should result in the number of households in the us, about 120M.Remember that there are actually 5 implicates for each household There are a few ways to deal with this: you can divide all of the weights by 5, or only use one of the implicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.wt0.sum()/1e6/5, df[df.implicate_id==1].wt0.sum()/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighting makes basic calculations much more difficult. Means, meadians and linear regresions will all require extra work to calculate properly. The equation for a weighted mean is fairly simple: \n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{ \\sum\\limits_{i=1}^n w_i x_i}{\\sum\\limits_{i=1}^n w_i}\n",
    "$$\n",
    "\n",
    "To calculate the weighted mean, we just need to multiply each of our values times the associated weight, then divide by the sum of the weights. Calculating the weighted mean for ``NORMINC``, we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.norminc*df.wt0).sum()/df.wt0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this value to the mean of the unweighted ``NORMINC``, which 7x larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.norminc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the unweighted mean is so mauch larger than the mean because there are 605 households in the sample that have more than \\\\$ 1M in normal income, and 85 with more than \\\\$10M. Because money variables are often very non-linear, these values will significantly skew statistics. This effect is also the reason that money values are most sensibly summarized with medians, rather than with means, as in our examples, because the mean of ``NORMINC``, at \\\\$762,000, is larger than 99\\% of the population and 90\\% of the surveyed households. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful trick is to re-arrange the equation for the weighted mean, by pre-dividing the weights by the sum of the weights. The result will be that the weights will sum to 1, and we can skip the division.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nwt0'] = df.wt0 / df.wt0.sum() # Normalized weight\n",
    "\n",
    "(df.norminc*df.nwt0).sum() # Simplier weighted mean of normal income. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, because the weights sum to 1, summing groups of weights gives us the proportion of that group in the total population. For instace, to caluculate the proportions of racial groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('racecl').nwt0.sum() # 1=White, 2=Non-white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the proportions in the unweighted statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('racecl').case_id.count()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a survey of finances, we are going to to a lot of work with variables representing money, and as we saw previously, it is better to use the median or quantiles to summarize money variables. Unfortunately, the algorithm for a weighted quantiles is much more complicated than the equation for weighted means, but there are several Python packages that implement weighted statistics, including:\n",
    "\n",
    "* [weightedstats](https://pypi.org/project/weightedstats/)\n",
    "* [wquantiles](https://pypi.org/project/wquantiles/)\n",
    "* [robustats](https://pypi.org/project/robustats/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightedstats as ws \n",
    "import wquantiles as wq\n",
    "\n",
    "ws.weighted_median( df.norminc, df.wt0), wq.median(df.norminc, df.wt0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More commonly you will want to compute medians of groups. This proedure is a bit more complicated in Pandas. First, we will define some helper functions, one for each of the first two helper packages. The weightedstats package doesn't like NA values, so there is a bit of extra work to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightedstats as ws \n",
    "import wquantiles as wq\n",
    "\n",
    "def wmedian(df, column_name, weights_name='wt0'):\n",
    "    \n",
    "    df = df.dropna(subset=[column_name,weights_name ])\n",
    "    \n",
    "    return ws.weighted_median( df[column_name], weights=df[weights_name])\n",
    "    \n",
    "def wmedian2(df, column_name, weights_name='wt0'):\n",
    "    \n",
    "    df = df.dropna(subset=[column_name,weights_name ])\n",
    "    \n",
    "    return wq.median( df[column_name], df[weights_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education class. 1=no high school diploma/GED, 2=high school diploma or GED,\n",
    "# 3=some college or Assoc. degree, 4=Bachelors degree or higher;\n",
    "df.groupby('edcl').apply(wmedian2, 'norminc') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Sample\n",
    "\n",
    "The odd way that these weighted median functions are called on groups is a bit disruptive, so it would be nice to use normal Pandas idioms instead. Fortunately we can, if we transform the weighted sample to an unweighted sample with random sampling. Using the Pandas ``.sample()`` method, we can draw a representative sample from the weighted sample and then just use regular statistics. Here, we will draw 1M samples from the weighted dataset and compute the group median incomes for ``EDCL``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(int(1e6),replace=True, weights=df.wt0)\n",
    "\n",
    "dfs.groupby('edcl').norminc.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few disadvantages with this technique:\n",
    "\n",
    "* It is a bit slow to generate the sampled dataset\n",
    "* The sampled dataset is very large. For 1M records, this dataset is 60Mb\n",
    "* The results are probablistic. \n",
    "\n",
    "Fortunately, these are not significant disadvantages. The first two should not matter much any modern computer, including laptops, and the third doesn't matter much for computing medians, because the coarseness of the dataset means that nearly any large sample will return the same median for most variables. For instance, let's compute the median of ``NORMINC`` for colleged educated households with three draws at each sample size. The values are identical for all draws with samples of 100,000  or more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ df.sample(int(sz),replace=True, weights=df.wt0).groupby('edcl').norminc.median().loc[4] \n",
    "     for _ in range(3) for sz in (10e5, 100e5, 1e6, 10e6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means, however, have a range of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "means=[ df.sample(int(sz),replace=True, weights=df.wt0).groupby('edcl').norminc.mean().loc[4] \n",
    "     for _ in range(3) for sz in (10e5, 100e5, 1e6, 10e6)]\n",
    "plt.hist(means);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if we average all of these different sampled values, we'll get a nearly identical result to the directly computed weighted mean, ( $\\pm .03 \\%$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df[df.edcl==4]\n",
    "direct_mean = (t.norminc*t.wt0).sum()/t.wt0.sum()\n",
    "sampled_mean = np.mean(means)\n",
    "\n",
    "direct_mean, sampled_mean, (direct_mean-sampled_mean)/direct_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sampling teach makes nearly all of the work with a weighted dataset much easier, and for sufficiently large samples, returns nearly identical results. It also greatly simplifies other techniques, such as regression. For future tutorials, we will use a sampling method for all exploratory work and will use weighted calculations for final, publishable results or for cases where the difference is negligible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* <a name=\"fnote1\">1</a> Hanna, Sherman D., Kyoung Tae Kim, and Suzanne Lindamood. “Behind the Numbers: Understanding the Survey of Consumer Finances.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, June 19, 2018. https://papers.ssrn.com/abstract=3199293.\n",
    "* <a name=\"fnote2\">2</a> LINDAMOOD, SUZANNE, SHERMAN D. HANNA, and LAN BI. “Using the Survey of Consumer Finances: Some Methodological Considerations and Issues.” The Journal of Consumer Affairs 41, no. 2 (2007): 195–214. https://www.jstor.org/stable/23860056. [PDF from ResearchGate. ](https://www.researchgate.net/publication/327050039_Behind_the_Numbers_Understanding_the_Survey_of_Consumer_Finances)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
